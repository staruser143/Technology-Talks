Adding **URL summarization** support to your app allows users to input a web link (e.g., a news article), and your app will:

1. ğŸ•·ï¸ Scrape the webpage  
2. ğŸ§¹ Extract the main article text (not ads, menus, etc.)  
3. âœï¸ Summarize it using your AI model

Letâ€™s walk through how to do this **step by step** in Python, with Streamlit integration.

---

### âœ… Step 1: Install Required Libraries

Youâ€™ll need:
- `requests` â€“ to fetch the webpage
- `beautifulsoup4` â€“ to parse HTML
- `trafilatura` â€“ excellent for extracting clean article text (recommended)
  - Or `newspaper3k` as an alternative

Install them:

```bash
pip install requests beautifulsoup4 trafilatura
```

> ğŸ” Alternative: `pip install newspaper3k` (but it can be fragile on some sites)

We'll use **`trafilatura`** â€” it's fast, accurate, and handles paywalls, comments, and ads well.

---

### âœ… Step 2: Create a Function to Extract Text from URL

```python
import requests
from trafilatura import fetch_url, extract

def extract_text_from_url(url):
    """
    Extract main article text from a URL using trafilatura.
    Returns cleaned text or an error message.
    """
    try:
        # Validate URL
        if not url.startswith("http://") and not url.startswith("https://"):
            return "âŒ Invalid URL. Please include http:// or https://"

        # Fetch and extract content
        downloaded = fetch_url(url)
        if downloaded is None:
            return "âŒ Failed to download the page. Check the URL or internet connection."

        text = extract(downloaded, include_comments=False, include_tables=True, include_formatting=False)
        
        if text is None or len(text.strip()) < 100:
            return "âŒ Could not extract meaningful text. The page may be empty or not article-based."

        return text.strip()

    except Exception as e:
        return f"âŒ Error: {str(e)}"
```

---

### âœ… Step 3: Update Your Streamlit UI

Add a new tab or input field for URL:

```python
# In your app.py, add a tab for URL input
tab1, tab2, tab3 = st.tabs(["ğŸ“ Paste Text", "ğŸ“ Upload PDF", "ğŸŒ Summarize URL"])

input_text = ""

with tab3:
    url = st.text_input("Enter Article URL:", placeholder="https://example.com/news-story")
    if st.button("Fetch & Extract Text", key="fetch_url"):
        if not url:
            st.warning("Please enter a URL.")
        else:
            with st.spinner("ğŸ“¡ Fetching article..."):
                input_text = extract_text_from_url(url)
            
            # Display result
            if input_text.startswith("âŒ"):
                st.error(input_text)
            else:
                st.session_state.fetched_text = input_text  # Save to session state
                st.success(f"âœ… Extracted {len(input_text.split())} words.")
                with st.expander("ğŸ“„ View Extracted Text"):
                    st.write(input_text)

# Allow fetched text to be summarized
if 'fetched_text' in st.session_state:
    input_text = st.session_state.fetched_text
```

> ğŸ” We use `st.session_state` so the text persists after switching tabs or clicking "Generate Summary".

---

### âœ… Step 4: Reuse Your Existing Summarizer

Now, when the user clicks **"Generate Summary"**, it will use the latest `input_text`, whether from:
- Text box
- PDF upload
- URL fetch

So your **"Generate Summary" button** logic stays the same:

```python
if st.button("âœ¨ Generate Summary"):
    if not input_text:
        st.warning("No text to summarize. Enter text, upload a PDF, or provide a URL.")
    else:
        with st.spinner("ğŸ§  Summarizing..."):
            summary = summarize_long_document(input_text)  # Your map-reduce function
        st.subheader("âœ… Summary")
        st.write(summary)
```

---

### âœ… Example: Test With a Real News Article

Try with:
```
https://www.bbc.com/news/world-us-canada-68920592
```

It should:
1. Fetch the article
2. Extract clean text (title + body)
3. Summarize it

---

### ğŸ”’ Optional: Add Rate Limiting & Safety

To avoid abuse or spamming:

```python
import time

# Add delay between requests
time.sleep(1)
```

And validate domains:
```python
from urllib.parse import urlparse

domain = urlparse(url).netloc
allowed_domains = ["bbc.com", "nytimes.com", "reuters.com", "example.com"]
if not any(d in domain for d in allowed_domains):
    st.warning("âš ï¸ Only trusted domains allowed in demo mode.")
```

---

### ğŸ†š Alternative: Use `newspaper3k`

If you prefer `newspaper3k`:

```python
from newspaper import Article

def extract_text_from_url(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text.strip()
    except Exception as e:
        return f"âŒ Failed: {str(e)}"
```

But `newspaper3k` can fail on many modern sites due to JavaScript-heavy content.

âœ… **Stick with `trafilatura`** â€” itâ€™s more robust.

---

### âœ… Features You Can Add

| Feature | How |
|-------|-----|
| ğŸ”¤ Show article title | `extract(..., with_metadata=True)['title']` |
| ğŸ•’ Read estimated time | `len(text.split()) / 200` â‰ˆ minutes |
| ğŸ“¥ Auto-fetch on paste | Detect URL in text input |
| ğŸ” Preview image | Use `og:image` meta tag |
| ğŸ§¹ Clean ads/scripts | `trafilatura` already does this |

---

### âœ… Final Notes

- âŒ Doesnâ€™t work with **JavaScript-heavy sites** (e.g., React apps) unless you use `selenium` or `playwright`
- âœ… Works great with **news, blogs, Wikipedia, Medium**
- âš ï¸ Some sites block scrapers â€” respect `robots.txt` and rate limits
- âœ… For production: use caching (`st.cache_data`) to avoid refetching

---

### âœ… Add Caching (Optional)

```python
@st.cache_data(ttl=3600)  # Cache for 1 hour
def fetch_url_cached(url):
    return extract_text_from_url(url)
```

Then call `fetch_url_cached(url)` instead.

---

### ğŸ‰ You're Done!

You now have a **fully functional URL summarizer** that can:
- Extract clean article text from links
- Handle errors gracefully
- Plug into your existing summarization pipeline

---

Let me know if you want:
- Support for **YouTube video transcripts** (via `pytube` + Whisper)
- Summarizing **Twitter/X threads**
- A **browser extension** version

You're building a powerful AI-powered research assistant! ğŸš€ğŸ§ 